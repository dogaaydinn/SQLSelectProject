# Logstash Pipeline Configuration for Employee Management System
# Process logs from FastAPI, NGINX, and PostgreSQL

input {
  # FastAPI application logs (JSON format)
  tcp {
    port => 5000
    codec => json
    tags => ["fastapi"]
  }

  # NGINX access logs (JSON format)
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    codec => json
    tags => ["nginx", "access"]
  }

  # NGINX error logs
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    tags => ["nginx", "error"]
  }

  # PostgreSQL logs
  file {
    path => "/var/log/postgresql/postgresql-*.log"
    start_position => "beginning"
    tags => ["postgresql"]
  }

  # Container logs via Docker logging driver
  udp {
    port => 5000
    codec => json
    tags => ["docker"]
  }

  # Syslog input
  syslog {
    port => 514
    tags => ["syslog"]
  }
}

filter {
  # Parse FastAPI logs
  if "fastapi" in [tags] {
    # Already in JSON format from Python logging
    mutate {
      add_field => { "log_type" => "application" }
      add_field => { "service" => "api-python" }
    }

    # Parse timestamp
    date {
      match => [ "timestamp", "ISO8601" ]
      target => "@timestamp"
    }

    # Extract HTTP status code category
    if [status_code] {
      ruby {
        code => "
          status = event.get('status_code').to_i
          if status >= 200 && status < 300
            event.set('status_category', 'success')
          elsif status >= 300 && status < 400
            event.set('status_category', 'redirect')
          elsif status >= 400 && status < 500
            event.set('status_category', 'client_error')
          elsif status >= 500
            event.set('status_category', 'server_error')
          end
        "
      }
    }

    # Parse request duration to numeric
    if [duration_ms] {
      mutate {
        convert => { "duration_ms" => "float" }
      }
    }

    # GeoIP lookup for IP addresses
    if [ip_address] {
      geoip {
        source => "ip_address"
        target => "geoip"
      }
    }
  }

  # Parse NGINX access logs
  if "nginx" in [tags] and "access" in [tags] {
    # Already in JSON format
    mutate {
      add_field => { "log_type" => "access" }
      add_field => { "service" => "nginx" }
    }

    # Parse response time to numeric
    if [request_time] {
      mutate {
        convert => { "request_time" => "float" }
      }
    }

    # Extract user agent information
    if [http_user_agent] {
      useragent {
        source => "http_user_agent"
        target => "user_agent"
      }
    }

    # GeoIP lookup
    if [remote_addr] {
      geoip {
        source => "remote_addr"
        target => "geoip"
      }
    }
  }

  # Parse NGINX error logs
  if "nginx" in [tags] and "error" in [tags] {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{NUMBER:pid}#%{NUMBER:tid}: \*%{NUMBER:connection_id} %{GREEDYDATA:error_message}"
      }
    }

    mutate {
      add_field => { "log_type" => "error" }
      add_field => { "service" => "nginx" }
    }
  }

  # Parse PostgreSQL logs
  if "postgresql" in [tags] {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{TZ} \[%{NUMBER:pid}\] %{WORD:user}@%{WORD:database} %{LOGLEVEL:level}:  %{GREEDYDATA:sql_message}"
      }
    }

    # Extract SQL query duration
    if [sql_message] =~ /duration: / {
      grok {
        match => {
          "sql_message" => "duration: %{NUMBER:query_duration_ms:float} ms"
        }
      }

      # Flag slow queries (> 100ms)
      if [query_duration_ms] and [query_duration_ms] > 100 {
        mutate {
          add_field => { "slow_query" => "true" }
        }
      }
    }

    mutate {
      add_field => { "log_type" => "database" }
      add_field => { "service" => "postgresql" }
    }
  }

  # Common filters for all logs

  # Remove unnecessary fields
  mutate {
    remove_field => [ "host", "agent", "ecs" ]
  }

  # Add environment information
  mutate {
    add_field => {
      "environment" => "${ENVIRONMENT:production}"
      "cluster" => "employee-management"
    }
  }

  # Fingerprint for deduplication
  fingerprint {
    source => ["message", "@timestamp", "service"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
    index => "logs-%{service}-%{+YYYY.MM.dd}"
    document_id => "%{[@metadata][fingerprint]}"

    # Authentication (uncomment if Elasticsearch has auth)
    # user => "${ELASTICSEARCH_USER:elastic}"
    # password => "${ELASTICSEARCH_PASSWORD:changeme}"
  }

  # Output slow queries to separate index
  if [slow_query] == "true" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
      index => "slow-queries-%{+YYYY.MM.dd}"
    }
  }

  # Output errors to separate index for alerting
  if [level] == "ERROR" or [level] == "CRITICAL" or [status_category] == "server_error" {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
      index => "errors-%{+YYYY.MM.dd}"
    }
  }

  # Output security events
  if "security" in [tags] or [path] =~ /\/auth\// {
    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST:elasticsearch:9200}"]
      index => "security-%{+YYYY.MM.dd}"
    }
  }

  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
}
